---
layout: post
title: An introduction to Variational Autoencoders-Background,Loss function and Application
date: 2018-10-15 12:32:00
categories: 深度学习
tags: Unsupervised-Learning Feature-Extractor
mathjax: true
figure: /images/VAE/mainfold_hypothesis.png
---

* content
{:toc}

## 前言

Variational autoencoders(VAE)是一类生成模型，它将深度学习与统计推断相结合，可用于学习高维数据$X$的低维表示$Z$。与传统自编码器不同，Variational autoencoders 假设$X$与$Z$都是满足某种分布假设的随机变量(向量)，因此Variational autoencoder 本质是对随机向量分布参数的估计(如均值，方差等矩估计)。在这个假设下，我们可以利用分布函数假设与预测参数进对$p(x\vert z)$与$p(z\vert x)$进行估计，用最大似然设计损失函数，并利用概率分布$p($x$\vert z)$来对$X$进行采样与生成。

本文旨在对VAE进行基于背景，损失函数以及应用方面的介绍。本文将先对VAE所需要的数学知识与基本假设进行简要描述，并在主体部分对文献[Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908)进行翻译，同时对该文章中省略或笔者认为叙述不清数学证明与显然性描述进行补全与解释。

本文写作过程中主要参考资料为:

* [Introduction to variational autoencoders](https://tensorchiefs.github.io/bbs/files/vae.pdf)
* [Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908)
* [Pattern Recognition and Machine Learning(Chap 1.6)](https://www.spiedigitallibrary.org/journals/Journal-of-Electronic-Imaging/volume-16/issue-4/049901/Pattern-Recognition-and-Machine-Learning/10.1117/1.2819119.short?SSO=1)
* [Wikipedia for some terms](https://www.wikipedia.org/)





## 数学知识与基本假设

### 流形假设,定义与性质

#### 流形的定义与性质

##### 直观理解

流形是局部具有欧几里得性质的空间，是曲线与曲面的推广。欧式空间就是一个最简单的流形实例。几何形体的拓扑结构是柔软的，因为所有的isomorphic会保持拓扑结构不变，而解析几何则是“硬”的，因为整体结构是固定的。光滑流形可以认为是两者之间的模型，它的无穷小结构是硬的，但是整体结构是软的。硬度是容纳微分结构的原因，而软度则可以成为很多需要独立的局部扰动的数学与物理模型。一般流形可以通过把许多平直的片折弯粘连而成，一个直观的理解就是生活中用的麻将席，麻将席整体是柔软的，但是其局部(一小片一小片竹板)则是硬的。

##### 严格定义

**流形定义为：**

假设$M$是豪斯多夫空间(即空间中任意两个点都可以通过邻域来分离)，假设对于任意一点 $x\in M$，都存在$x$的一个邻域，这个邻域同胚(即存在两个拓扑空间中的双连续函数)于$m$维欧式空间$R^m$的一个开集，就称$M$是一个$m$维流形。

#### 流形假设
流形假设就是真实世界中的高维数据(比如说图像)位于一个镶嵌在高维空间中的低维流形上。比如我们获得了二维随机向量$(x,y)$的一组采样:

$$
(1,1),(2,2),(3,3),...,(n,n)
$$

那么这组数据就是在二维空间中的一条线上(镶嵌在二维空间中的一维流形),即该组二维数据可以由$x=t,y=t,t\in R$生成。

流形假设即数据$x=(x_1,....,x_N)$由k维空间中的$y=(y_1,...,y_k)$经连续函数(不一定光滑，不一定线性，也不一定是双射)所构成:

$$
x_i=f_i(y_1,...,y_k),\forall i \in \{1,..,N\}
$$

一个直观理解是手写数字的生成模型，如图所示。手写数字由$(v_1,v_2)$所生成，在$v_2$上的线性变换即为手写数字的旋转，在$v_1$上的线性变换则对应了手写数字的放缩。

![fig1](/images/VAE/mainfold_hypothesis.png)

流形假设是非常重要的假设，该假设可以部分解释为什么深度学习work。同时当考虑到符合流形假设的一些问题的时候（如图像处理等），这个假设可以给我们一些直观，并能够在工程中进行应用(如VAE)。

### 潜变量空间模型假设

该部分是对文献[Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908) Section1.1的翻译，提前放在此处用于与流形假设对比从而辅助理解。

#### 潜变量空间模型介绍

当训练一个生成模型的时候，生成数据不同维度之间的依赖越强，那么模型就越难进行训练。以生成手写数字0-9为例，如果数字的左半部分是数字*5*的左半部分，那么右半部分就不可能是数字*0*的右半部分，不然生成的图像显然不是一个真实的数字。直观上，一个好的生成模型应该在生成每一个像素所对应值之前先决定到底要生成哪个数字(这样可以避免生成四不像的目标)，而所谓的**决定**就是潜变量。也就是说，在我们的模型想要生成数字之前，将先从集合$\{0,...,9\}$中随机选一个数字$z$，然后确保模型画出来的结果与$z$相匹配。这里z被称为**潜在的**，因为我们直接得到的是模型所生成的结果，而并不需要知道是哪个$z$生成了这个结果。对于潜变量，我们可以用计算机视觉相关的技术进行推断。

当我们说我们的生成模型"代表"了数据集时，我们需要确保对数据集中的每一个$X$，存在至少一个潜变量$z$可以让模型通过$z$生成与$X$非常相似的结果。

#### 潜变量空间假设的数学描述与直观意义

该假设的数学化描述如下:

假设我们有生成空间$\mathcal{X}$，同时有一个潜变量空间$Z$，并在$Z$上定义了随机向量$z$的概率密度函数$P(z)$，那么我们可以依据$P(z)$对潜变量向量进行采样。假设我们有一族函数$\{f(z;\theta)\},z\in Z,\theta\in \Theta$，满足:

$$
f:Z\times \Theta \mapsto \mathcal{X}
$$

$f$是一个确定的映射，但如果$z$是随机向量，同时$\theta$是确定向量，那么$f(z,\theta)$是生成空间$\mathcal{X}$中的随机向量。给定一个训练集$\{X_i\},\forall i,X_i\in \mathcal{X}$，我们的目标是最大化:

$$
P(X_i)=\int P(X_i\vert z;\theta)P(z)dz
$$

注意在这里我们用$P(X_i\vert z;\theta)$表示$P(X_i=f(z;\theta))$，即给定潜变量空间的潜变量向量$z$时，模型生成$X_i$的概率。这个目标函数的直观是最大似然理论，即如果模型生成训练样本$X_i$的概率最大时，它更有可能生成与训练样本相似的样本，而不是生成与训练样本完全不相似的样本。

在变分自编码器领域中，我们对于生成空间的分布假设一般是高斯分布，即:

$$
P(X\vert z;\theta)=N(X\vert f(z;\theta),\sigma ^2 *I)
$$

该概率密度函数的均值为$f(z;\theta)$，同时生成空间的各维度互相独立，且方差为$\sigma ^2$。在高斯分布假设下，有一些$z$可以生成仅仅是与$X$相似的样本而并不与$X$一致。总之，在训练过程中，我们的模型并不会生成与训练样本$X$完全一样的输出。

#### 高斯分布假设

高斯分布假设的意义在于，我们可以用梯度下降算法(或者其它的优化技术)，取定潜变量$z$后，通过 **令$f(z;\theta)$靠近$X$** 来最大化目标函数$P(X)$，并在训练过程中逐渐让训练集数据的生成概率变大。

$$
\text{令$f(z;\theta)$靠近$X$的解释如下:}\\

\text{在高斯分布假设下:}\\

-log(p(X))=C_1 + C_2*\Vert f(z,\theta)-X\Vert
$$
因此最大化$-log(P(X))$等价于最大化$X$与$f(z,\theta)$的二阶矩。如果高斯分布不满足，比如$P(X\vert z)$是一个Dirac delta function，那么我们就没有办法最大化概率函数，因为此时$f(z;\theta)$所预测的并不是均值，而是以概率1收敛到$P(X\vert z)$的预测，此时我们没有办法定义梯度。

#### Bernoulli分布假设

假设生成空间$X$是二值化的，那么$P(X\vert z)$可以进行Bernoulli分布假设，其参数为$f(z;\theta)$，此时最大似然如下：

$$
P(X\vert z)= log(\Pi p(x_{ij}\vert z))=\sum_{i,j}log(f(z,\theta)_{ij})
$$

此时同样可以计算梯度并用梯度下降法优化模型。

### 信息论

这里我将补充一些信息论的基本知识。

考虑一个离散随机变量$x$，我们研究如下问题:当我们获得了这个随机变量的一个观测的时候，我们到底得到了多少信息？信息量可以度量 **"令你惊讶的程度"**，即如果有两个互斥事件$A,B$，有一个先验信息为 $P(A)<P(B)$，然后我们观测到A事件发生了，那么我们所获得的信息一定会多于观测到B事件发生所获得的信息。因此，对$x$信息的度量$h(x)$应该与$x$的分布函数$p(x)$相关。我们可以做出如下两条显然的假设:

$$
if\ p(x,y)=p(x)p(y),then\ h(x,y)=h(x)+h(y)\\
if\ p(x)<p(y),\ then\  h(x)>h(y)
$$

基于这两条假设，我们可以有:

$$
h(x)=-log_{2}p(x)
$$

我们沿用香农信息论的记号，以2作为对数基底。

基于$h(x)$我们考虑一个新问题，假设一个信息发送者想要把一个随机变量$x$的值传送给一个接收者，那么在这个传输过程中，传输信息量的期望可以基于$x$的分布函数$p(x)$进行如下计算：

$$
H[x]=-\sum_xp(x)log_{2}p(x)
$$

$H[x]$被称为随机变量$x$的熵(entropy)，注意这里我们令$p(x)log_{2}p(x)=0,if\ p(x)=0$。

这里我们不加证明地给出一个结论，这个结论在
