---
layout: post
title: A Survey of Semi-supervised Deep Learning Method 
date: 2019-7-17 12:32:00
categories: 机器学习
tags: Deep-Learning, SSL
mathjax: true
---

* content
{:toc}

**FengHZ‘s Blog首发原创**

基于标注监督的深度学习技术已经取得了巨大的成功，并极大推动了计算机视觉(*CV*)，自然语言处理(*NLP*)等领域的进步。一般而言，只要构造一个足够大的数据集，且这个数据集拥有高质量的标注，我们就可以构造模型容量(*Capacity*)足够大的神经网络，通过反向传播(*Back-Propagation*)与随机优化算法(*SGD,RMSProp,Adam*)训练神经网络，并在测试集上达到足够高的泛化性能。但是，获取高质量的数据不是一件容易的事情。深度学习所需要的数据燃料以万计，获得较为精确的标注需要大量的人力，尤其是当数据标注涉及专家知识的时候，获得标注则变得极为昂贵。

半监督学习(*semi-supervised learning*)正是应对这种情况而生的方法。对于很多任务，获取原始数据成本低廉，而获取标注成本较高。在这些任务上，半监督学习算法可以同时利用少量有标注数据与大量无标注数据进行训练，其结果可以大大提高模型的泛化能力，甚至在某些任务上接近全监督学习的效果。

本文主要介绍半监督学习的发展历史以及现在主流深度半监督学习算法的四个派别：

* 基于深度生成模型的半监督学习算法
* 基于差异学习的半监督学习算法
* 基于图模型的半监督学习算法

  
其中，每一个派别内都会有许多不同的分支，对于每一个分支我们也将进行细分叙述。因参考文献较多，我们将参考文献放于文末，同时本文会保持持续更新。







## 半监督学习：历史与发展[1]

想象一个情景，假如我们有大量无标注数据，同时有少量有标注数据，如何利用无标注数据呢？一个最自然的想法是，我们用有标注数据去训练一个较为简单的模型，用模型对于无标注数据进行预测，然后把那些置信度不高的结果(如预测概率在0.5附近的模型)挑出来，对这些结果打上标注，再训练一个稍微复杂一些的模型。如此反复直到模型取得最优效果。这种方法称为"主动学习"(Active Learning)，其目的是使用尽量少的"查询"(query)来获得尽量好的性能。

主动学习引入了额外的专家知识，同时通过与外界的交互将部分未标记样本转化为有标记样本，它是一种间接的半监督学习方法。能否在不引入新专家知识的情况下，用未标记样本提高泛化性能呢？在一些假设的情况下，答案是可以的。通过以下四种不同的假设，我们可以得到四个半监督学习算法的一般范式(paradigm).

1. 未标记数据与标记数据来源于同一数据分布，数据分布存在潜变量模型假设，在潜变量空间内存在簇状聚类结构，同一个簇的数据属于同一类别
   
   在该假设下，我们可以用生成模型进行半监督学习。给定有标注样本集 $D_{l}=\{(x_1,y_1),\ldots,(x_l,y_l)\}$以及无标注样本集 $D_{u}=\{x_{l+1},\ldots,x_{l+u}\}$，同时假设所有样本独立同分布，对应的生成模型为$\mathcal{G}$, 生成潜变量分布为 $z_i\sim \mathcal{N}(\mu_i,\Sigma_i)$, 那么 我们可以最大化 $D_{l}\cup D_{u}$的对数似然

   $$
   LL(D_{l}\cup D_{u}) = \sum_{(x_j,y_j)\in D_{l}} \ln p(x_j,y_j\vert z_j) +\sum_{x_j\in D_{u}}\ln p(x_j \vert z_j)
   $$

   生成模型将标注看成是潜变量之一，在有标注的时候将其视作已知潜变量进行生成，在无标注的时候利用生成模型的自监督损失函数进行自动拟合。一般而言，数据越多，对统计量的预测也就越准确，生成模型表现也就越好。我们利用大量数据去拟合一个优秀的生成模型，将标注看成是低维空间的潜变量，采用少量有标注样本引导模型学习这种潜变量，从而完成半监督学习。

2. 未标记数据与标记数据来源于同一数据分布，彼此相似的数据有很高的可能性属于同一类别
   
   该假设是基于图的半监督学习方法的基本假设。基于图的半监督学习方法需要先用无监督方法学习(或定义)一个近邻矩阵(*Affinity Matrix) W*, 其中$W_{i,j}$表示第i个样本与第j个样本的相似度. 依据我们的假设直观，模型对于相似样本的预测也应该是相近的，定义模型对样本*x*的预测为 $f(x)$, 我们可以定义模型在近邻矩阵*W*下的能量函数

   $$
   E(f) = \frac{1}{2} \sum_{i=1}^{m}\sum_{j=1}^{m}W_{i,j}(f(x_i)-f(x_j))^2=f^TLf \tag{1}
   $$

   其中$f=(f(x_1),\ldots,f(x_m))^T$,[**L为W的Laplacian矩阵**](https://fenghz.github.io/A-tutorial-on-spectral-clustering/#3-%E5%9B%BE%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%9F%A9%E9%98%B5%E4%BB%A5%E5%8F%8A%E5%AE%83%E4%BB%AC%E7%9A%84%E7%89%B9%E6%80%A7).

   对于有标注的数据，我们给定约束$f(x_l)=y_l$, 并将其代入$(1)$. 对于无标注数据，我们要求其在$(1)$下达到最小能量函数，并对$f$进行优化. 优化问题可以转化为一个图割问题解决.

3. 未标记数据与标记数据来源于同一数据分布，不同标签数据的划分边界是**低密度**的。
   
   在该类假设下，分类器试图找到将有标记样本划分开，同时划分区域穿过数据低密度区域的划分超平面。此时，未标记数据起到了揭示数据聚类结构的作用，如下图所示。

   ![1.png](/images/semi-supervised/1.png)

    半监督SVM算法是一种利用低密度划分的半监督学习算法。

4. 未标记数据与标记数据的源数据具有多个视图，不同视图对应的标记空间相似
   
   在该类假设下，一个数据由多个视图组成，不同视图对应的标注空间相同，我们可以利用多视图的"相容互补性"，通过协同训练来进行半监督学习：首先在每个视图上基于有标记样本训练分类器，让每个分类器挑选自己最有把握的样本赋予伪标记，将伪标记提供给其他视图的分类器作为新增样本重新训练，这种"互相学习，共同进步"的过程不断迭代进行，直到分类器不再发生变化。

   协同训练可以推广到单视图，通过构造不同的学习算法，或者不同的数据采样，甚至是不同的参数设置产生不同的学习器，利用学习器之间的"分歧"相互提供伪标记样本提升泛化性，因此这种方法范式也被称为"基于差异的半监督学习方法".

对半监督学习方法的评价有两种模式，一是纯半监督学习，二是直推学习。纯半监督学习是指直接预测测试集的数据作为最后的结果，而直推学习则是指预测用于训练的未标注数据，将该预测值作为训练结果。如果用于训练的未标注数据的预测结果较为优秀，则说明模型确实利用到了未标注数据来改进目标预测结果，如果测试集数据的预测结果较好，则说明模型利用未标注数据提高了泛化性。

半监督学习的研究一般始于地球科学与遥感领域，第一篇与半监督学习相关的文章发表于 IEEE Trans on Geoscience and Remote Sensing, 题目为 "The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon." 在二十一世纪初，随着现实中对未标注数据利用的重大需求涌现，半监督算法蓬勃发展。从2008年以来，在每年*ICML*大会所评选的十年最佳论文中，半监督学习的四大范形中的三篇代表论文均获奖。现今的深度半监督方法仍属于以上四个范式，但是深度学习的加入使得模型的泛化能力提升了一个层次。

## 基于深度生成模型的半监督学习算法

深度生成模型发展到现在有三个派别，它们分别是对抗生成网络(GAN[2]), 变分自编码器(VAE[3]), 以及自回归像素生成网络(PixelCNN,PixelRNN[4]), 由于自回归生成网络训练速度较慢, 深度半监督学习常用前两者作为基本生成模型.

### Semi-supervised GAN

### Semi-supervised VAE

## 基于差异学习的半监督学习算法

深度半监督学习算法的另外一个套路是人为制造我们不想看到的差异，并对差异施加正则化约束从而让网络在训练过程中减小该差异，并证明这种差异学习可以提高网络在(纯)半监督学习任务上的泛化性能。

### Data Augmentation Method

### Consistency Regularization Method

### Mixup Method

## 基于图模型的半监督学习算法


















## Reference

[1] 周志华. 机器学习[M]. Qing hua da xue chu ban she, 2016.

[2] Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[C]//Advances in neural information processing systems. 2014: 2672-2680.

[3] Kingma D P, Welling M. Auto-encoding variational bayes[J]. arXiv preprint arXiv:1312.6114, 2013.

[4] Salimans T, Karpathy A, Chen X, et al. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications[J]. arXiv preprint arXiv:1701.05517, 2017.