---
layout: post
title: An Introduction to the Differential Privacy on Deep Learning
date: 2021-5-5 10:49:00
categories: 机器学习
tags: Differential Privacy, Deep Learning, Privacy
mathjax: true
---

数据是人工智能的燃料，优秀的深度学习模型需要依靠大量高质量数据集进行训练。然而，随着模型精度的不断提升，对于个人隐私的泄露现象也变得越发严重。此外，随着互联网企业的扩展，用户数据开始担任重要生产资料的角色，成为各大垄断企业的护城河。欧盟，作为反对互联网垄断的桥头堡，同时也作为隐私保护的急先锋，在2018年正式施行法案《通用数据保护条例》(General Data protection Regulation, GDPR)。GDPR主张个人对数据的四项权利，请求权，拒绝权，修正权和删除、遗忘权。请求权，即个人有权了解其个人数据是否被处理，哪些个人数据以怎样的方式被处理以及进行了哪些数据处理操作；拒绝权，即个人有令人信服的合法理由，可禁止进行某些数据处理操作，比如个人可拒绝以营销为目的的个人数据处理。遗忘权，即个人有权寻求删除其个人数据的影响，比如用个人的微博，抖音数据训练的推荐算法，能够把个人的影响给忘掉。此外，GDPR还对数据的传输有明确的要求，比如欧盟境内的数据不得在境外被使用。







那么，一个自然的问题是，如何让人工智能模型的训练过程能够符合数据保护条例，保护个人隐私呢？依据GDPR的要求，首先数据的存储必须满足去中心化，而模型则需要在去中心化的数据库上进行分布式训练。联邦学习[1]是应对这种要求的解决方案，它允许模型在本地数据库上训练，并构建一个全局的调度器，通过对不同本地模型的更新进行汇总（即FedAvg算法），获得全局模型。得到的全局模型能够利用所有本地数据的信息，并得到更好的模型精度。其次，GDPR要求个人能够控制其数据对于模型的影响。人工智能模型是通过归纳所有数据信息构建的，它的输出结果是否会泄露个人隐私呢？差分隐私（Differential Privacy)[2]系统地探讨并解决了这一问题。在本篇Blog中，我们对应用于深度学习中的差分隐私技术进行入门式的介绍，包括差分隐私的动机与定义，满足差分隐私要求的梯度下降方法，以及如何对差分隐私的隐私保护程度进行计算。因为我们的主要目的是将差分隐私应用于隐私保护场景，而非对差分隐私进行研究，在本篇Blog中我们会略过一些数学证明的过程，将重心聚焦到差分隐私的应用上。

## 差分隐私的动机，背景以及基本定义

### 隐私的基本定义

在讨论差分隐私之前，我们先对“什么是隐私”这一问题进行讨论。在不同的考量下，隐私的定义也不一样。目前普遍比较接受的定义是：“单个用户的某一些属性” 可以被看做是隐私。注意该说法所强调的是“单个用户”，也就是说，如果是一群用户的某一些属性，那么可以不看做隐私。譬如，医院发布调查报告说，抽烟的人会有更高几率得肺癌，这个不泄露任何隐私。但是如果医生说，张三因为抽烟，所以有x\%的概率得肺癌，这就是个人隐私的泄露。如果我们拥有一个数据库，那么对精确的个体信息的查询与检索都会泄露隐私，因此对于个人数据的加密是最基本的保护隐私策略。然而，就算我们对个人数据进行了加密，对一群用户的某些属性的查询，以及对查询结果进行加工与建模，也就是“数据分析”，往往也会泄露个人隐私。而相比于数据加密，对数据分析的过程进行隐私保护因其不对称性更加具有难点，因为数据加密过程中，解密者与攻击者往往是两方，但是数据分析的过程中，分析师与“隐私攻击者”则是一方。

我们先来举几个例子，看看什么样的数据分析行为会侵犯隐私，以及什么样的行为看似不会侵犯隐私，但是通过一连串叠加也会侵犯个人隐私。首先，在直觉上，如果查询的数据库特别大，而且我们规定只允许查询摘要形式的信息或者统计形式的信息，这种数据分析方法看起来保护了个人隐私。但是，如果我们知道某个个体的信息包含在数据库中，我们就可以利用一种叫差分攻击的方法得到个体的信息。比如，如果我们已知**X**的信息在某个大型医疗数据库中。那么我们可以查询：有多少人患血友病，以及条件查询，有多少个不叫**X**的人患白血病。这样经过差分，我们就得到了**X**是否患血友病的信息。对于这种情况，如果我们引入某种监督机制，比如有一个监督者禁止这种不安全的查询，或者有监督者在必要的时候对数据库进行匿名处理，似乎可以避免上述的差分攻击，是否有一个监督机制能保证数据分析过程中，隐私不被泄露吗？答案是没有，原因有二：(1) 禁止查询的决定本身也会带来隐私泄露。比如国家不公布第七次人口普查数据，或者从2006年开始不公布中国的基尼系数，这本身也有某些信息。(2) 可以将单次破坏隐私的查询拆分成一系列查询，而对于每次查询，很难判断是否破坏隐私。最后，由于数据分析师的先验知识背景非常多样，因此数据分析师的辅助信息也会将一些不破坏隐私的查询变得破坏隐私，这就带来了第二种隐私泄露的形式，即由于数据库之外的辅助信息带来的隐私泄露。比如我们知道邻居**X**在某一天去了医院，而我们得到的匿名医院数据库中，在这一天的数据条目特别少，那么**X**的个人信息也会以某种高概率泄露。比如作为**X**的邻居，我们发现他常常去买蛋糕。但是有一段时间他忽然开始买不含糖的面包了，如果我是一个医生，我可能会猜想他患了糖尿病，这就是辅助信息带来的隐私泄露。

综上所述，差分攻击与辅助信息是数据分析过程中隐私泄露的主要诱因，而引入监管也不能保证隐私。针对这种状况，差分隐私提出了一种数据分析过程的隐私保护定义，即**对于数据库数据的任何分析，以及根据分析结果与其他信息进行的进一步的合并加工，都不会泄露个人隐私。**

### 差分隐私的动机与基本形式



## References
[1] Konečný J, McMahan B, Ramage D. Federated optimization: Distributed optimization beyond the datacenter[J]. arXiv preprint arXiv:1511.03575, 2015.

[2] Dwork C, Roth A. The algorithmic foundations of differential privacy[J]. Foundations and Trends in Theoretical Computer Science, 2014, 9(3-4): 211-407.